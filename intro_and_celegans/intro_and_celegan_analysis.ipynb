{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168d936d",
   "metadata": {},
   "source": [
    "# Workshop on Network Science\n",
    "\n",
    "##### Authors: Mauricio Barahona and Robert Peach\n",
    "\n",
    "##### Motiviation\n",
    "\n",
    "We are surrounded by systems that are hopelessly complicated - complex systems - and behind each complex system there is an intricate network that encodes the interactions between the individual components.\n",
    "Knowing the systems components, such as the neurons in the brain, is insufficient to derive the collective behaviour of millions interacting. Given the important role complex systems play in our daily life, in science and in economy, their understanding, mathematical description, prediction, and eventually control is one of the major intellectual and scientific challenges of the 21st century. We will never understand complex systems unless we develop a deep understanding of the networks behind them.\n",
    "\n",
    "![title](images/network.gif)\n",
    "\n",
    "\n",
    "Network science is a relatively new discipline when compared to traditional subjects: Its exact beginning is up for debate, but has only really emerged as a separate discipline during the 21st century. The development of tools to interrogate networks, and the areas that have benefitted from network analyses, have grown exponentially over recent years. \n",
    "\n",
    "Note: There is no real difference between a **graph** and  **network**. However, when we model a real, existing system as a graph, we tend to call it a network.\n",
    "\n",
    "\n",
    "\n",
    "##### What to expect from this workshop?\n",
    "\n",
    "We want to give a first introduction to networks, show you a diverse set of applications, and introduce standard methods for working with networks in Python. We will certainly not be comprehensive, but instead zone in on a few interesting use-cases, with a slight tendency towards neuroscience. We will lightly touch on the following:\n",
    "1. Introduction to the NetworkX package in Python.\n",
    "2. The structure of the Caenorhabditis elegans connectome.\n",
    "3. Dynamics on networks (e.g., synchronisation, epidemics).\n",
    "4. Network Neuroscience.\n",
    "\n",
    "##### What might we learn?\n",
    "\n",
    "We don't necessarily want you to learn every detail about the examples we give, instead we want to give you a flavour for networks such that you might see links with your own research now or in the future.\n",
    "\n",
    "1. Familiarity with NetworkX Python package.\n",
    "2. Understanding the importance of approaches to visualisation of networks.\n",
    "3. How to apply algorithmic tools to examine network structure, including:\n",
    "    - Community detection\n",
    "    - Centrality\n",
    "    - Clustering\n",
    "4. Perform dynamics on networks:\n",
    "    - Synchronisation models\n",
    "    - Epidemic simulations\n",
    "5. Aspects of network construction with neuroscientific data:\n",
    "    - Functional connectivity data\n",
    "    - Pose estimation data\n",
    "\n",
    "\n",
    "### Fill out the following form (just once!)\n",
    "\n",
    "Before we get started, please complete the following google form (if you consent to taking part). \n",
    "\n",
    "https://docs.google.com/forms/d/e/1FAIpQLSepRdz-GrlIDgqlR-ArVlZShMVcdu72NFleX2JcB8qu-s9b8g/viewform?usp=sf_link\n",
    "\n",
    "Together, we will analyse the resulting social network later!\n",
    "\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Before you run these notebooks, there are a few dependencies (packages that you need to also have installed). These should all be defined in the requirements.txt file. \n",
    "\n",
    "Just access the main folder for this code via the terminal and run:\n",
    "> pip install -r requirements.txt\n",
    "\n",
    "If there are any further problems, please let us know and we will do our best to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65897dc7",
   "metadata": {},
   "source": [
    "# Introduction to Networks\n",
    "\n",
    "Let us begin our foray into network science with a short introduction to NetworkX. Installation details for the package can be found here: https://networkx.org/documentation/stable/install.html.\n",
    "\n",
    "Note: we have chosen NetworkX because its comprehensive library of functions and wider familiarity in the scientific community, but emphasise that there are many other network packages (of which some are considerably faster) including:\n",
    "\n",
    "- igraph: https://igraph.org/ Collection of network analysis tools with emphasis on efficiency, portability and ease of use.\n",
    "- graph-tool: https://graph-tool.skewed.de/ Graph-tool is an efficient Python module for manipulation and statistical analysis of graph (backend in C++)\n",
    "- NetworKit: https://networkit.github.io/ A growing open-source toolkit for large-scale network analysis. \n",
    "- snap.py: https://snap.stanford.edu/snappy/ Python interface for SNAP. SNAP is a general purpose, high performance system for analysis and manipulation of large networks. \n",
    "- pathpy: https://www.pathpy.net/ Open Source python package providing higher-order network analytics for time series data.\n",
    "- hypernextx: https://pnnl.github.io/HyperNetX Extending networks to hypergraphs.\n",
    "- brainconn: https://brainconn.readthedocs.io/en/latest/ Python package for graph theoretic analysis of neuroimaging data.\n",
    "- and many, many more.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d2659",
   "metadata": {},
   "source": [
    "### Lets get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601d121",
   "metadata": {},
   "source": [
    "Lets first make some Python package imports to help us through our journey through the wonders of Network Science and Graph theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "from scipy.sparse.linalg import eigs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can first generate a classical graph from NetworkX to play with\n",
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dba23b",
   "metadata": {},
   "source": [
    "Notice below how the edges can be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2660ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the edge information\n",
    "G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02931a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its easy to draw wth networkX! It default uses a spring layout\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also define a dictionary for plotting positions\n",
    "pos = nx.spring_layout(G) # defining the layout with springs\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we can also use other layouts... \n",
    "nx.draw(G, pos=nx.circular_layout(G), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e642301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network has an associated adjacency matrix A\n",
    "A = nx.adjacency_matrix(G) # extract adjacency matrix\n",
    "plt.imshow(A.todense()); plt.colorbar(); # plot matrix \n",
    "A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b65608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get the incidence matrix B\n",
    "B = nx.incidence_matrix(G, weight='weight') # extract adjacency matrix\n",
    "plt.imshow(B.todense()); plt.colorbar();  # plot matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97cf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add edges\n",
    "G.add_edge(10, 16, weight=1)  # specify edge data\n",
    "\n",
    "# lets visualise the added edges\n",
    "nx.draw(G, pos, with_labels=True) \n",
    "nx.draw_networkx_edges(G, pos, edgelist=[(10,16)], edge_color='r', width=4) # draw additional edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca09b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can identify nodes that are central according to their degree\n",
    "centrality = nx.degree_centrality(G)\n",
    "nx.draw(G, pos, node_color=list(centrality.values())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22b703",
   "metadata": {},
   "source": [
    "# Caenorhabditis elegans connectome \n",
    "\n",
    "Now that we have had a brief introduction to NetworkX, lets jump into a particular example and continue our introduction of networks.\n",
    "\n",
    "Here, we will go through an analysis of the Caenorhabditis elegans connectome.\n",
    "C. elegans is the only organism for which the wiring diagram of its complete nervous system has been mapped with reasonable accuracy at the cellular level. Despite this structural information, which has been available for decades, it still proves difficult to understand the system, e.g, resolving the functional involvement of specific neurons in defined behavioural responses.\n",
    "\n",
    "We will use data that we have reconstructed (with some difficulty) from the following article with the inclusion of muscles https://www.nature.com/articles/nature24056.\n",
    "The connectome is composed of directed connections from one neuron to another neuron in accordance with their biological influence.\n",
    "Each node falls into one of four categories:\n",
    "1. Sensory neurons\n",
    "2. Inter neurons\n",
    "3. Motor neurons\n",
    "4. Muscles\n",
    "\n",
    "Therefore, we can model the nematode nervous system as a *directed network* whose nodes include neurons and muscles, and whose links represent the electrical and chemical synaptic connections between them, including neuromuscular junctions. The weights of the edges correspond to the number of synaptic connections between a pair of neurons.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50640f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data from a pickle object\n",
    "with open('celegan_data.pickle', 'rb') as handle:\n",
    "    [G, _, groups, neuron_class, colours] = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see what data we have!\n",
    "G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75ac47",
   "metadata": {},
   "source": [
    "## 1. Visualisation of C. Elegan Structure \n",
    "\n",
    "Lets plot a basic visualisation with the commands we saw earlier, i.e., the spring layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the network using the spring layout we saw earlier!\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G,pos=pos,node_color=colours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7df85e",
   "metadata": {},
   "source": [
    "What information does this provide us? \n",
    "1. We can see that the muscles (in red) are located at the periphery.\n",
    "2. The interneurons (blue) seem to be more central to the structure, followed by the sensory neurons (orange) and then motor neurons (green).\n",
    "\n",
    "But its still not particularly informative. I don't personally get a sense of network structure. **Can we do better?** \n",
    "\n",
    "We could provide the exact layout defined in the physical structure of the C. Elegan, but 3-dimensional network structures often don't translate well to 2-dimensional visualisations. Moreover, the physical layout might hide aspects of the network dynamics. \n",
    "\n",
    "Instead, we are going to implement a special layout of the C.Elegan network based on flow paths (https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066). This part might be quite involved,  but **don't feel like you need to follow everything here**.\n",
    "\n",
    "Our aim: \n",
    "1. Arrange the nodes (neurons) along the vertical axis conveys information about the directionality of the signal flow in the network\n",
    "2. Arrange the nodes (neurons) along the horizontal axis to convey information about the strength of connectivity regardless of directionality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a0d87",
   "metadata": {},
   "source": [
    "#### 1. Vertical coordinate\n",
    "\n",
    "We want to arrange the neurons so that for every synaptically connected pair of neurons, the difference in $z$ between a presynaptic neuron *i* and a postsynaptic neuron *j* is as close to one as possible.\n",
    "\n",
    "We can define an energy function of the connectivity matrix which we need to minimise:\n",
    "\n",
    "<center>$E = \\frac{1}{2} \\sum_{i,j=1}^{n} W_{ij}(z_i - z_j - sgn(A_{ij}- A_{ji}))^2$</center>\n",
    "\n",
    "which is the sum of the gap junction and chemical connectivity matrices, and\n",
    "the symmetrized connectivity matrix $W_{ij}$ , which satisfies $W_{ij}$  = ($A_{ij}$  +$A_{ji}$ )/2. Lets get some intuition for this formula:\n",
    "1. If two neurons aren't connected then $W_{ij}=0$ and thus they contribute nothing to the sum.\n",
    "2. If two neurons are connected we want to find coordinates $z_i$ and $z_j$ such that  $z_i- z_j \\rightarrow 0$, i.e., we want to place them as close together as possible. \n",
    "3. The weight $W_{ij}$ indicates how ‘important’ it is that $z_i - z_j - sgn(A_{ij}- A_{ji})$ is small.\n",
    "4. The term $sgn(A_{ij}- A_{ji})$ imposes a height difference. Reversing the direction of the edge turns this value from positive to negative.\n",
    "\n",
    "Since the above equation is a nice quadratic function, we can find the minimum by setting the derivative of the above expression to zero (how is everyones A-level maths doing?). In doing so we find,\n",
    "\n",
    "<center>$Lz = b$</center>\n",
    "\n",
    "where $z$ is our $N \\times 1$ vector of coordinates, $b_i = \\sum_{j=1}^{n} W_{ij} sgn(A_{ij}$ -$A_{ji})$ and $L = D-W$ is the graph Laplacian which is defined in terms of a diagonal matrix $D$ that contains the number of synaptic terminals on corresponding neurons,\n",
    "\n",
    "<center>$D_{ij} = \\delta_{ij} \\sum_{k=1}^{n} W_{ik}$</center>\n",
    "\n",
    "where a unique solution to this equation can be found by using the pseudoinverse.\n",
    "\n",
    "<center>$z = L^{\\dagger}b$</center>\n",
    "\n",
    "We skipped a lot of steps here transitioning from the energy function to the Laplacian formulation for brevity - you can find the details here [1]. \n",
    "\n",
    "Lets just recap a little on what we did: **We have used principles of the weighted network structure to bring nodes closer together that are linked with heavier weighted edges, and imposed nodes with out-going edges to be vertically higher in the network visualisation.**\n",
    "\n",
    "Note 1: The Laplace operator is a differential operator given by the divergence of the gradient. The Laplacian describes how a function differs from its average locally, which also holds for graph Laplacians $L$ above.\n",
    "\n",
    "Note 2: The sgn() function simply indicates that we take the sign of the values in the matrix (positive or negative).\n",
    "\n",
    "[1] 1. Carmel L, Harel D, Koren Y (2004) Combining hierarchy and energy for drawing directed graphs. IEEE\n",
    "Trans Vis Comput Graphics 10:46–57. doi:10.1109/TVCG.2004.1260757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us reverse the direction of the edges (to point from muscles towards sensory neurons)\n",
    "G_ = G.reverse()\n",
    "\n",
    "# first we will extract the adjacency matrix of our graph\n",
    "A = np.array(nx.to_numpy_matrix(G_))\n",
    "    \n",
    "# define a matrix W as the undirected adjacency matrix of our graph   \n",
    "W = 0.5 * (A + A.T)                                                                                 \n",
    "     \n",
    "# extract the weighted degree associated with each neuron\n",
    "weighted_degree = np.array(W.sum(1)).flatten()\n",
    "\n",
    "# create diagonal matrix of weighted degree\n",
    "D = np.diag(weighted_degree)        \n",
    "\n",
    "# defining the minimum to target\n",
    "b = np.array((W * np.sign(A - A.T)).sum(0)).flatten()                                               \n",
    "\n",
    "# defining the graph Laplacian\n",
    "L = (nx.laplacian_matrix(nx.Graph(W))).toarray()\n",
    "\n",
    "# taking pseudo inverse of Laplacian and dot product with b\n",
    "z = np.array(np.linalg.pinv(L).dot(b)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c95f12",
   "metadata": {},
   "source": [
    "#### 2. Horizontal coordinate\n",
    "\n",
    "To find the horizontal coordinates, we use the Laplacian, $L$, normalized by the number-of-terminals matrix $D$,\n",
    "\n",
    "<center>$L_{norm} = D^{-1/2}LD^{-1/2}$.</center>\n",
    "\n",
    "We are interested in taking the eigenvector with the second lowest eigenvalue of $L_{norm}$, we denote this as $v_2$. Then the horizontal coordinates for our visualisation are found by weighting the eigenvector value for each node by its weighted degree (i.e. the weight of all incoming and outgoing connections),\n",
    "\n",
    "<center>$x = D^{-1/2}v_2$ </center>\n",
    "\n",
    "This method produces an aesthetically appealing drawing because each neuron is placed in the weighted centroid of its neighbors. Thus strongly coupled neurons tend to be co-located and nodes that are play a more fundamental role within the structure of the network are placed more centrally along the x-dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f130fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normalized graph Laplacian\n",
    "L_norm = nx.normalized_laplacian_matrix(nx.Graph(W))\n",
    "\n",
    "# perform eigendecomposition with scipy package\n",
    "V, U = sc.sparse.linalg.eigs(L_norm, which=\"SM\", k=3)\n",
    "\n",
    "# take second and third eigenvectors\n",
    "vs = U[:, 1:]\n",
    "\n",
    "# normalisation matrix - square root of the degree\n",
    "D_sqrt_inv = np.diag(1 / np.sqrt(np.diag(D)))\n",
    "\n",
    "# dot product each eigenvector with the normalisation matrix\n",
    "v2 = D_sqrt_inv.dot(vs[:, 0])\n",
    "v3 = D_sqrt_inv.dot(vs[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a450ce",
   "metadata": {},
   "source": [
    "#### Define coordinates and replot\n",
    "\n",
    "Now that we have our vertical and horizontal coordinates (finally..) we can bring them together and plot our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each node and assigned x,y coordinates in dictionary\n",
    "pos = {}\n",
    "for i in G:\n",
    "    pos[i] = (np.real(v2[i]), z[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6eb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# plot \n",
    "nx.draw(G, pos = pos, node_color=colours,node_size=weighted_degree*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd7ba9",
   "metadata": {},
   "source": [
    "#### Visualisation Summary\n",
    "\n",
    "The plot above seems a little easier to undertand in comparison to our first attempts! The distance along the vertical coordinate corresponds roughly to the number of synapses from sensory to motor neurons—the signal flow depth of the network. Neuronal position on the horizontal plane represents the connectivity closeness of neurons in the combined chemical and electrical synapse network. \n",
    "\n",
    "Thus, the visualisation represents not the physical placement of neurons in the worm but signal flow and closeness in the network. Such visualization reveals that motorneurons and some interneurons segregate into two lobes along the first horizontal axis: the right lobe contains motorneurons in the ventral cord and the left lobe consists of neck neurons. The bi-lobe structure suggests partial autonomy of motorneurons in the ventral cord and neck. Interneurons that could coordinate the function of the two lobes can be easily identified by their central location.\n",
    "\n",
    "Of course, defining this layout was a little involved and requires some thinking, but it **highlights the importance of visualisation in network analysis.** You should also start to think carefully about how you visualise your networks in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b230f",
   "metadata": {},
   "source": [
    "## 2. Small world properties of C.Elegan\n",
    "\n",
    "Now that the network visualiation is out of the way, lets look at some network measures! We can now investigate properties that may describe the efficiency of signal transmission across the gap junction network. \n",
    "\n",
    "#### Characteristic path length\n",
    "\n",
    "The geodesic distance, $d_{ij}$, between two neurons in the network is the length of the shortest network path between them (respecting the directionality in our directed network). The network path is measured by the number of connections that are crossed rather than by physical distance. The average geodesic distance over all pairs of neurons is the characteristic path length.\n",
    "\n",
    "<center>$L = \\frac{1}{N(N-1)}\\sum_{i,j:i\\neq j}d_{ij}$</center>\n",
    "\n",
    "This global measure describes how readily or rapidly a signal can travel from one neuron to another since it is simply the average distance between all neurons.\n",
    "\n",
    "Watts DJ, Strogatz SH (1998) Collective dynamics of ‘small-world’ networks. Nature 393: 440–442. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3021da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(G, weight='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31189143",
   "metadata": {},
   "source": [
    "How does this compare with a random graph with the same degree sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7724639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract in and out degree sequence of C.Elegan graph\n",
    "din = list(d for n, d in G.in_degree()) \n",
    "dout = list(d for n, d in G.out_degree())\n",
    "\n",
    "# use configuration model to generate a random graph with the same in and out degree sequence\n",
    "G_random = nx.directed_configuration_model(din, dout)\n",
    "\n",
    "nx.average_shortest_path_length(G_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the distribution of across many repeated instantiations of the graph\n",
    "n_repeats = 100\n",
    "spl = []\n",
    "for i in range(100):\n",
    "    G_random = nx.directed_configuration_model(din, dout)\n",
    "    spl.append(nx.average_shortest_path_length(G_random))\n",
    "\n",
    "# plot distribution\n",
    "plt.hist(spl,label='Random networks'); plt.xlim([1.8, 2.2])\n",
    "plt.xlabel('Characteristic path length');plt.ylabel('Frequency')\n",
    "\n",
    "# plot real c.elegan average shortest path length\n",
    "plt.axvline(x=nx.average_shortest_path_length(G),color='r',label='C. Elegan network')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c328536",
   "metadata": {},
   "source": [
    "What is this telling us? We see that the average path length of random graphs are shorter than the C.Elegan. Suggesting that the C.Elegan network topology has evolved to have longer routes in its nervous system. Why could this be?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132bd36",
   "metadata": {},
   "source": [
    "#### Clustering coefficient\n",
    "\n",
    "The clustering coefficient $C_i$ measures the density of connections among an average nodes neighbors.\n",
    "There are several definitions of clustering for directed graphs in the literature, but here we use the clustering of the out-connected neighbors since it captures signal flow emanating from a given neuron.\n",
    "\n",
    "<center>$C = \\frac{1}{N} \\sum_i C_i$    and   $C_i = \\frac{E(N_i)}{k_i(k_i-1)}$</center>\n",
    "\n",
    "where $E(N_i)$ is the number of connections between out-neighbors of neuron $i$, $k_i$ is the number of out-neighbors of $i$, and $C_i$ measures the density of connections in the neighborhood of neuron $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cea574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual node clustering coefficient unweighted\n",
    "clustering = nx.clustering(G)\n",
    "\n",
    "# average across all nodes\n",
    "global_clustering = np.mean(list(clustering.values()))\n",
    "print(global_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd8eae",
   "metadata": {},
   "source": [
    "How does this compare to a random graph with the same degree sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract in and out degree sequence of C.Elegan graph\n",
    "din = list(d for n, d in G.in_degree()) \n",
    "dout = list(d for n, d in G.out_degree())\n",
    "\n",
    "# use configuration model to generate a random graph with the same in and out degree sequence\n",
    "G_random = nx.directed_configuration_model(din, dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaae131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual node clustering coefficient unweighted\n",
    "clustering = nx.clustering(nx.Graph(G_random))\n",
    "\n",
    "# average across all nodes\n",
    "global_clustering_rand = np.mean(list(clustering.values()))\n",
    "print(global_clustering_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the distribution of across many repeated instantiations of the graph\n",
    "n_repeats = 100\n",
    "c_dist = []\n",
    "for i in range(100):\n",
    "    G_random = nx.directed_configuration_model(din, dout)\n",
    "    c = nx.clustering(nx.Graph(G_random))\n",
    "    clustering = np.mean(list(c.values()))\n",
    "    c_dist.append(clustering)\n",
    "\n",
    "# plot distribution\n",
    "plt.hist(c_dist,label='Random networks'); #plt.xlim([0.05, 0.25])\n",
    "plt.xlabel('Global clustering coefficient');plt.ylabel('Frequency')\n",
    "\n",
    "# plot real c.elegan average shortest path length\n",
    "plt.axvline(x=global_clustering,color='r',label='C. Elegan network')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329d1a9",
   "metadata": {},
   "source": [
    "The average clustering is much lower in the random graph than the C.Elegans. This suggests that something about the structure of the C.Elegan graph that deviates from chance. Higher than chance clustering may play a key role in mediating its control over the C.Elegan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b1e19",
   "metadata": {},
   "source": [
    "## 3. Important neurons in the C.Elegan\n",
    "\n",
    "\n",
    "There is no absolute definition of importance in a network. However, centrality measures attempt to capture aspects of node 'importance'. Centrality measures are among the most widely used indices based on network data. They generally reflect a unit's prominence; in different substantive settings, this may be its structural power, status, prestige, or visibility. Studies often use network-based centrality measures in efforts to account for interunit differences in behavior or attitudes.\n",
    "\n",
    "There are four well-known centrality measures: degree, betweenness, closeness and eigenvector - each with its own strengths and weaknesses. The main point we want to make is that the analytical usefulness of each depends heavily on the context of the network, the type of relation being analyzed and the underlying network morphology. We don’t want to leave you with the impression that one is better than another - only that one might serve your research goals better than another.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(12,6))\n",
    "\n",
    "# we will apply the four centrality measures\n",
    "funcs = [nx.degree_centrality, nx.betweenness_centrality, nx.closeness_centrality, nx.eigenvector_centrality]\n",
    "\n",
    "for centrality_function, ax in zip(funcs,fig.axes):\n",
    "    \n",
    "    centrality = centrality_function(G) # computing centrality    \n",
    "    \n",
    "    node = max(centrality , key=centrality.get) # getting most central node\n",
    "    print(G.nodes[node])\n",
    "    \n",
    "    centrality = np.asarray(list(centrality.values())) # converting to array\n",
    "    \n",
    "    centrality = centrality/np.max(centrality) # normalising for comparison\n",
    "    \n",
    "    # plot with centrality measures\n",
    "    nx.draw_networkx_nodes(G, pos = pos, node_color=centrality, node_size=centrality*100,  ax=ax)    \n",
    "    ax.set_title(centrality_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b686690",
   "metadata": {},
   "source": [
    "So we can see that the interneurons called AVAR and AVAL are the most central across almost all measures. **This neuron has been implicated in motor control and when ablated significantly reduces motor function in the C. Elegan.**\n",
    "\n",
    "In fact lets look at a few of the top nodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(12,6))\n",
    "\n",
    "# we will apply the four centrality measures\n",
    "funcs = [nx.degree_centrality, nx.betweenness_centrality, nx.closeness_centrality, nx.eigenvector_centrality]\n",
    "\n",
    "central_neurons = pd.DataFrame()\n",
    "central_neurons_type = pd.DataFrame()\n",
    "\n",
    "for centrality_function, ax in zip(funcs,fig.axes):\n",
    "    \n",
    "    centrality = centrality_function(G) # computing centrality    \n",
    "    \n",
    "    top_nodes = np.array(sorted(centrality , key=centrality.get))[::-1] # ordering nodes by centrality\n",
    "    #print(G.nodes[node])\n",
    "        \n",
    "    # plot with centrality measures\n",
    "    nx.draw_networkx_nodes(G, pos = pos, nodelist = top_nodes[10:], node_color='k', node_size=4,  ax=ax)    \n",
    "    ax.set_title(centrality_function)\n",
    "\n",
    "    # plot with centrality measures\n",
    "    nx.draw_networkx_nodes(G, pos = pos, nodelist = top_nodes[:10], node_color='r', node_size=40,  ax=ax)    \n",
    "    ax.set_title(centrality_function)\n",
    "    \n",
    "    # extracting top 10 nodes for each centrality function\n",
    "    central_neurons[centrality_function.__name__] = [G.nodes[u]['name'] for u in top_nodes[:10]]\n",
    "    central_neurons_type[centrality_function.__name__] = [G.nodes[u]['type'] for u in top_nodes[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_neurons_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137c4db",
   "metadata": {},
   "source": [
    "We can see that a lot of interneurons appear as the most central nodes regardless of centrality function!\n",
    "\n",
    "All are command interneurons involved in the locomotory circuit. The centrality of command interneurons may indicate that in the C. elegans nervous system, signals can propagate efficiently from various sources towards these neurons and away from them, and that they are in a good position to integrate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f63907",
   "metadata": {},
   "source": [
    "### Pagerank\n",
    "\n",
    "Now lets do a deep dive into pagerank (this is very similar to eigenvector centrality but for directed networks). PageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links. It was originally designed as an algorithm to rank web pages (in fact you may have heard of a little known spin-out company called Google that was founded on this algorithm). We choose this measure because its purposed for directed networks, whilst other measures such as eigenvector centrality are defined on undirected networks.\n",
    "Pagerank is based on random walks. Simply put: *If a large number of random-walks on a network end up at the particular node then  it must be important.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0119cca",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "0 & 0.5 & 0.5\\\\\n",
    "0.5 & 0.5 & 0\\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "First we must compute the transition matrix, which is just a row-stochastic matrix (each row sums to 1) and tells us about the probability of transitioning from node $i$ to node $j$ (see above example). However, directed networks have a small problem: some nodes are sinks (no outgoing edges) and some are sources (no incoming edges). \n",
    "\n",
    "<img src=\"images/sink_in_graph.png\" width=\"250\">\n",
    "\n",
    "<center>Directed networks can have sink nodes.</center>\n",
    "\n",
    "\n",
    "\n",
    "Since the transition matrix represents a Markov chain, we don't want to get trapped in sinks. Therefore, for PageRank to converge to a unique solution (i.e., a unique stationary distribution in a Markov chain), the transition matrix must be irreducible. In other words, it must be that there exists a path between every pair of nodes in the graph.\n",
    "\n",
    "\n",
    "We can use a damping factor (or sometimes known as teleportation constant when thought about in different terms) such that the random-walker can randomly jump to another node in the network. This can be invoked using 'alpha' in the computation of the google matrix function in networkX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract transition matrix\n",
    "transition_matrix = nx.google_matrix(G.reverse(),alpha=0.85)\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f6a12",
   "metadata": {},
   "source": [
    "Page rank can be computed either iteratively or algebraically. However, with large graphs, the iterative method (power method) is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59379644",
   "metadata": {},
   "source": [
    "#### Linear algebra approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3250b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigendecomposition of transposed matrix\n",
    "eigenvalues, eigenvectors = eigs(transition_matrix.T, k=1, which='LM')\n",
    "\n",
    "# normalise the eigenvectors\n",
    "pagerank = abs(eigenvectors)/abs(eigenvectors).sum()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(G, pos = pos, node_color=colours, node_size=pagerank*50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab23fb",
   "metadata": {},
   "source": [
    "#### NetworkX approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G.reverse())\n",
    "pagerank = np.array(list(pagerank.values()))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(G, pos = pos, node_color=colours, node_size=pagerank*50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fecc83",
   "metadata": {},
   "source": [
    "#### Power iteration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "# define vector of ones (this could be anything)\n",
    "r = np.ones((A.shape[0], 1))\n",
    "\n",
    "converged = False\n",
    "numIter  = 0\n",
    "\n",
    "# loop over until the r vector doesn't update any longer\n",
    "while not converged:\n",
    "    \n",
    "    # apply transition matrix to r\n",
    "    r_new = (transition_matrix.T).dot(r)\n",
    "    \n",
    "    # check for convergence\n",
    "    if (linalg.norm(r-r_new) < 0.0001):\n",
    "        converged = True\n",
    "    else:\n",
    "        # update r with the new r \n",
    "        r = r_new\n",
    "    \n",
    "    # count the iterations required\n",
    "    numIter += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the centrality values\n",
    "pagerank = np.array(r/r.sum())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(G, pos = pos, node_color=colours, node_size=pagerank*50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893e61e",
   "metadata": {},
   "source": [
    "## 4. Modules/communities in C. Elegan structure\n",
    "\n",
    "The complexity of biological, social, and engineering networks makes it desirable to find natural partitions into clusters (or communities) that can provide insight into the structure of the overall system and even act as simplified functional descriptions. A long standing technical challenge in network science is the automated discovery of communities — groups of nodes that are strongly connected or that share similar features or roles.\n",
    "\n",
    "Community detection is a rich and challenging problem, partly because it is not very well posed: what exactly do we mean by a community? In most cases, communities are defined as non-overlapping groups of nodes such that there are more edges within groups than between them, but this definition still leaves open many possibilities, and there are correspondingly many computational approaches. Moreover, there can exist multiple hierarchies of partitions as we see in the image below.\n",
    "\n",
    "<img src=\"images/multiscale_communities.png\" width=\"400\">\n",
    "\n",
    "There are many algorithms for identifying community partitions in networks, which generally fall into three classes (although of course there is overlap between them):\n",
    "1. Optimization methods, whereby the algorithm assigns scores to each possible division of the network. e.g. modularity.\n",
    "2. Statistical inference, whereby communities are not merely a feature of the network structure but a primary driver of it, i.e., the network has been generated from some statistical distribution.\n",
    "3. Methods that rely on links between community structure and dynamical processes taking place on networks, particularly random walks.\n",
    "\n",
    "\n",
    "For simplicity we will just implement a couple of algorithms that are readily available in networkX: modularity and kernigham-lin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995699dc",
   "metadata": {},
   "source": [
    "##### Greedy modularity maximization\n",
    "\n",
    "The algorithm begins with each node in its own community and repeatedly joins the pair of communities that lead to the largest modularity until no futher increase in modularity is possible (a maximum). \n",
    "\n",
    "But what is modularity? Modularity is a quality function that tells us the 'goodness' of a given partition! It is defined as,\n",
    "\n",
    "\\begin{align*}\n",
    "Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij}  - \\gamma \\frac{k_i k_j}{2m} \\right) \\delta(c_i, c_j)\n",
    "\\end{align*}\n",
    "\n",
    "where $m$ is the number of edges, $A$ is the adjacency matrix, $k_i$ is the degree of node $i$, $\\gamma$ is the resolution parameter, and $ \\delta(c_i, c_j)$ is 1 if $i$ and $j$ are in the same community (else 0).\n",
    "\n",
    "But how can we interpret this? Networks with large modularity have dense connections between nodes within a community, but sparse (few) connections between nodes in different communities. Our equation is computing the difference  between the actual number of edges between node $i$ and $j$ and the expected number of edges between them (if our network was random).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fef818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "# identify optimal partition\n",
    "partition = greedy_modularity_communities(G)\n",
    "\n",
    "# define colour scheme\n",
    "color = iter(cm.rainbow(np.linspace(0, 1, len(partition))))\n",
    "\n",
    "# loop over and plot each partition in a different colour\n",
    "plt.figure(figsize=(12, 8))\n",
    "for community in partition:\n",
    "    c = next(color)\n",
    "    nx.draw_networkx_nodes(G, pos = pos, nodelist=list(community),  node_color=c, node_size=pagerank[list(community)]*50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d2d7c",
   "metadata": {},
   "source": [
    "Recent work has proved that a universally best algorithm for community detection can not exist and that different partitions may describe different aspects of the structure! Therefore its always good to use different approaches here.\n",
    "\n",
    "Can you guys think of what makes a good community structure? What about multiscale structures? How would you build an algorithm for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7dc12",
   "metadata": {},
   "source": [
    "#### Girvan Newman algorithm\n",
    "\n",
    "The Girvan–Newman algorithm detects communities by progressively removing edges from the original graph. The algorithm removes the “most valuable” edge, traditionally the edge with the highest betweenness centrality, at each step. As the graph breaks down into pieces, the tightly knit community structure is exposed and the result can be depicted as a dendrogram. \n",
    "\n",
    "Note: betweenness centrality is defined as the proportion of times that a node lies on a shortest path given all shortest paths in the network. Shortest paths are defined by edge length! Therefore we need to take the inverse of edge weights.\n",
    "\n",
    "Below we just iterate through $n$ subdivisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_inv = G.copy()\n",
    "\n",
    "for u,v in G_inv.edges:\n",
    "    if 'weight' in G_inv[u][v].keys():\n",
    "        G_inv[u][v]['weight'] = 1/G_inv[u][v]['weight']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93787647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import girvan_newman\n",
    "\n",
    "partition_generator = girvan_newman(G_inv.to_undirected())\n",
    "n_cuts = 5\n",
    "\n",
    "for division in range(n_cuts):\n",
    "    partition = next(partition_generator)\n",
    "    \n",
    "    # define colour scheme\n",
    "    color = iter(cm.rainbow(np.linspace(0, 1, sum(1 for _ in partition))))\n",
    "\n",
    "    # loop over and plot each partition in a different colour\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for community in partition:\n",
    "        c = next(color)\n",
    "        nx.draw_networkx_nodes(G_inv, pos = pos, nodelist=list(community),  node_color=c, node_size=pagerank[list(community)]*50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb000fe8",
   "metadata": {},
   "source": [
    "#### Kernighan-Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import kernighan_lin_bisection\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "# identify optimal partition (this algo only works on undirected networks)\n",
    "partition = kernighan_lin_bisection(G.to_undirected())\n",
    "\n",
    "# define colour scheme\n",
    "color = iter(cm.rainbow(np.linspace(0, 1, sum(1 for _ in partition))))\n",
    "\n",
    "# loop over and plot each partition in a different colour\n",
    "plt.figure(figsize=(12, 8))\n",
    "for community in partition:\n",
    "    c = next(color)\n",
    "    nx.draw_networkx_nodes(G, pos = pos, nodelist=list(community),  node_color=c, node_size=pagerank[list(community)]*50000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
